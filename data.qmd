---
title: Data Cleaning
toc: true
draft: false
---

![](images/data-cleaning-page.png)

# Primary Data Source

The primary dataset used in this analysis comes from Montgomery County, Maryland's open data portal, specifically the "Crash Reporting - Drivers Data" dataset. This dataset is publicly available and can be accessed at [Montgomery County's Data Portal](https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Drivers-Data/mmzv-x632). The data is collected through the Automated Crash Reporting System (ACRS) of the Maryland State Police and includes reports from multiple law enforcement agencies: Montgomery County Police, Gaithersburg Police, Rockville Police, and the Maryland-National Capital Park Police.

The dataset was created and is maintained as part of Montgomery County's commitment to public safety and transparency, particularly in support of their Vision Zero initiative (as indicated by the dataset's keywords). Vision Zero is a strategy to eliminate all traffic fatalities and severe injuries while increasing safe, healthy, equitable mobility for all.

The data collection serves multiple purposes:

- To track and analyze traffic collision patterns across the county
- To inform policy decisions regarding traffic safety measures
- To provide transparency about traffic incidents to the public
- To support the coordination between different law enforcement agencies in the county

The dataset is updated weekly and contains detailed information about traffic collisions occurring on county and local roadways. Each record represents a driver involved in a collision, with 39 variables capturing various aspects of the incident, including:

### Key Variables

- Temporal and Spatial Information: Crash Date/Time, Location (Latitude/Longitude), Road Name
- Environmental Conditions: Weather, Surface Condition, Light, Traffic Control
- Driver Details: Driver At Fault, Injury Severity, Driver Substance Abuse, Driver Distracted By
- Vehicle Information: Vehicle Make, Model, Year, Body Type, Damage Extent

# Additional Data Sources

## Pavement Condition Index

The dataset, titled **Pavement Condition Index**, is provided by the **Department of Transportation of Montgomery County, Maryland**, and can be accessed through the Montgomery County open data portal at [data.montgomerycountymd.gov](https://data.montgomerycountymd.gov). This dataset was created to analyze and report on the conditions of pavements across the county's roadways. 

- **Purpose of Collection**
  - To evaluate and monitor the pavement condition of all 5,200 lane miles of roadways in Montgomery County.
  - Support county transportation planning and maintenance activities by providing numerical condition scores.

- **Attribution**
  - Collected and curated by the Montgomery County Department of Transportation.
  - Maintained and updated biennially with metadata recently updated on **June 11, 2023**.

### Data Files and Variables

The dataset includes the following variables, which provide details about pavement conditions. The most relevant variables are highlighted below:

#### Key Variables
- **Pavement Condition Index (PCI)**:
  - A numerical score between 1 and 100, representing the condition of the pavement.
  - **Score Categories**:
    - **0-30**: Poor condition.
    - **31-60**: Fair condition.
    - **61-80**: Good condition.
    - **81-100**: Very good condition.
  
- **Pavement Distress Types**:
  - Includes 19 types of distresses, such as cracking, potholes, environmental impacts, and utility cuts.
  
#### Additional Variables (Summarized):
- **Roadway ID**: Unique identifier for each roadway segment.
- **Street Name**: Name of the street being evaluated.
- **Segment Length**: Length of the road segment in miles.
- **Last Inspected Date**: Date of the most recent condition assessment.
- **Road Category**: Classifies roads (e.g., arterial, residential).

### Access and Use Information

- The dataset is publicly available and intended for use by the general public, researchers, and policymakers.
- It is categorized as **Non-Federal**, with no specific license provided.

### Additional Notes
- **Update Frequency**: The dataset is updated every two years to reflect changes in pavement conditions.
- **Limitations**: The dataset does not include real-time updates or historical comparisons across multiple update cycles.

## Maryland Highway Mileage Reports
The dataset, titled **Annual Highway Mileage Reports**, is provided by the **Maryland Department of Transportation State Highway Administration (MDOT SHA)**. This dataset combines comprehensive information about state, toll, county, and municipal roadway systems in Montgomery County, Maryland, focusing on three key report types: DSD9 (Detailed Surface Data), FCAVMT (Functional Class Annual Vehicle Miles Traveled), and FCMI (Functional Class Mileage Information) from 2017 to 2022.

- **Purpose of Collection**
  - To track and analyze highway infrastructure across different functional classifications in Montgomery County.
  - Support transportation planning and policy decisions by providing detailed metrics on road usage and capacity.
  - Monitor changes in road infrastructure and vehicle miles traveled over time.

- **Attribution**
  - Collected and maintained by the Maryland Department of Transportation State Highway Administration.
  - Represents official state records of highway infrastructure and usage.

### Data Files and Variables
The dataset combines three main report types, each providing distinct insights into the county's highway system:

#### Key Components
- **DSD9 (Detailed Surface Data)**:
  - Mainline mileage (total, divided, non-divided)
  - Annual vehicle miles
  - Lane mileage (auxiliary spurs, mainline, total)
  - Square yards of surfacing
  
- **FCAVMT (Functional Class Annual Vehicle Miles Traveled)**:
  - Tracks vehicle miles traveled across different road classifications
  - Separates data into rural and urban categories
  - Includes various road types from Interstate to Local roads
  
- **FCMI (Functional Class Mileage Information)**:
  - Documents road mileage by functional classification
  - Covers both rural and urban road networks
  - Categorizes roads from Interstate to Local systems

#### Classification Variables
- **Road Categories**:
  - Interstate
  - Other Freeways
  - Principal Arterial
  - Minor Arterial
  - Major Collector
  - Minor Collector
  - Local Roads
  
Each category is further subdivided into rural and urban designations.

### Access and Use Information
- The dataset represents official state records and is used for planning and administrative purposes.
- Data is structured to support analysis of transportation infrastructure trends from 2017 to 2022.

### Additional Notes
- **Time Coverage**: The dataset spans six years (2017-2022), providing recent historical context.
- **Geographical Scope**: Focuses specifically on Montgomery County, Maryland.
- **Integration**: Combines three distinct report types to provide a comprehensive view of the county's highway system.
- **Limitations**: Earlier years may have fewer data categories as reporting requirements expanded over time.

## Montgomery County Crime Dataset

The dataset, titled **Crime**, is provided by the **Montgomery County Police Department** and can be accessed through Montgomery County's open data portal at dataMontgomery. This dataset contains detailed crime statistics and incident reports, utilizing the National Incident-Based Reporting System (NIBRS) classification standards.

The primary purpose of this dataset is to provide public access to comprehensive crime statistics in Montgomery County, supporting law enforcement analysis and transparency initiatives while enabling data-driven policy decisions regarding public safety. The data is collected and maintained by the Montgomery County Police Department and compiled using the EJustice records-management system. The data is updated daily, with metadata last updated on December 7, 2024.

### Data Files and Variables

The dataset includes detailed information about reported crimes through various key components. The incident information includes unique identifiers such as Incident ID and CR Number, along with temporal data including dispatch date/time, and start and end date/time. Each incident is classified using NIBRS codes and can include up to three crime name categories per incident, along with offense codes and the number of victims involved.

Location data is extensively documented, including police district name and number, detailed block address components, city, state, and zip code information. Geographic coordinates are provided through latitude and longitude, and law enforcement jurisdictional information is included through sector, beat, and PRA designations.

### Access and Use Information

The dataset is made available to the public through multiple format options, including CSV, RDF/XML, JSON, and XML. The data coverage begins from July 1st, 2016, and can be accessed through the Montgomery County open data portal. To maintain privacy standards, all personal information is excluded from the public dataset to protect victims' privacy.

### Additional Notes

The dataset is updated daily, with quarterly refreshes to reflect any changes in investigation status. Users should be aware of several important limitations: the information may be preliminary and subject to change, incidents may include attempted crimes alongside completed ones, and crime classifications may be updated as investigations progress. The data may contain mechanical or human errors, and while arrest information is included, all arrested persons are presumed innocent until proven guilty in a court of law.

This comprehensive dataset serves as a valuable resource for understanding crime patterns and law enforcement activities in Montgomery County, while maintaining appropriate privacy protections and data quality standards. The frequent updates and multiple format options make it a useful tool for researchers, policy makers, and the general public interested in local crime statistics and trends.

## Data Cleaning Process

Our data cleaning process is implemented in our [load_and_clean_data.R](/scripts/load_and_clean_data.R) script. Here's how to use it and what it does:

### Setup and Requirements

First, ensure you have all required packages installed:

```r
install.packages(c("tidyverse", "lubridate", "skimr", "here"))
```

### Running the Cleaning Script

The script can be run in two ways:

```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Option 1: Source directly in your Quarto document
source(here::here("scripts", "load_and_clean_data.R"), echo = TRUE)

# Option 2: Load the already cleaned data
clean_data <- readRDS(here::here("dataset", "cleaned_dataset_full.rds"))

# Or load the exploration subset
subset_data <- readRDS(here::here("dataset", "cleaned_dataset.rds"))
```

### Cleaning Operations

Our cleaning script performs the following operations:

1. Removes columns with more than 80% missing values
2. Converts the case number to proper character format
3. Standardizes date/time formats using lubridate
4. Handles missing values in the Route Type column
5. Removes duplicate entries

## Data Integration Process

When combining the datasets, we faced several challenges that required careful handling. The geographic alignment between the crash data and the PCI dataset was particularly challenging due to differences in road name formatting and segmentation. We developed a matching algorithm that standardized road names and used geographic coordinates to ensure accurate joining of the datasets, as shown in the following code:

```{r, eval=FALSE, echo=TRUE}
# Standardize road names in both datasets
standardize_road_names <- function(road_name) {
  road_name %>%
    str_to_upper() %>%
    str_replace_all(c(
      "AVENUE" = "AVE",
      "BOULEVARD" = "BLVD",
      "STREET" = "ST",
      "ROAD" = "RD",
      "HIGHWAY" = "HWY"
    )) %>%
    str_trim()
}

# Clean and prepare datasets for matching
crash_data <- crash_data %>%
  mutate(
    Road.Name = standardize_road_names(Road.Name),
    location_key = paste(Road.Name, Municipality)
  )

pci_data <- pci_data %>%
  mutate(
    StreetName = standardize_road_names(StreetName),
    location_key = paste(StreetName, City)
  )

# Perform fuzzy matching for unmatched roads
library(stringdist)
unmatched_roads <- crash_data %>%
  filter(!location_key %in% pci_data$location_key)

# Match using string distance with a threshold
fuzzy_matches <- stringdistmatrix(
  unmatched_roads$Road.Name,
  pci_data$StreetName,
  method = "jw",
  p = 0.1
) %>% as.data.frame()
```

The integration of temporal data presented another challenge, particularly when combining crash reports with annual highway data. We implemented a time-based joining strategy that matched crash incidents with the corresponding annual data, ensuring that we maintained the temporal integrity of our analysis:

```{r, eval=FALSE, echo=TRUE}
# Prepare temporal joins for highway data
crash_data <- crash_data %>%
  mutate(
    crash_year = year(Crash.Date.Time),
    crash_month = month(Crash.Date.Time)
  )

highway_data <- highway_data %>%
  mutate(report_year = year(report_date))

# Join datasets with temporal alignment
combined_data <- crash_data %>%
  left_join(
    highway_data,
    by = c("crash_year" = "report_year", "Road.Name" = "highway_name")
  ) %>%
  # Handle cases where highway data is from previous year
  group_by(Road.Name) %>%
  fill(c(traffic_volume, lane_width, surface_type), .direction = "downup") %>%
  ungroup()

# Validate temporal alignment
temporal_coverage <- combined_data %>%
  group_by(crash_year) %>%
  summarise(
    crash_count = n(),
    has_highway_data = sum(!is.na(traffic_volume)) / n()
  )
```

For the highway mileage, we have several steps for data cleaning. Our approach began with the
implementation of a sophisticated road classification system that addresses the varying naming
conventions found across different reporting years. We developed a standardize_road_class function that employs regular expressions to match different descriptions of
the same road types, creating a unified and consistent classification scheme throughout the
dataset.

The handling of missing values formed a crucial component of our data cleaning process. We
approached this challenge with context-specific solutions for different types of data. For instance,
gaps in the Annual Average Daily Traffic (AADT) measurements were filled using median values to
maintain statistical relevance, while missing lane mile data was conservatively set to zero. All other
metric calculations incorporated robust aggregation methods to ensure accurate results even in the
presence of incomplete data.

Temporal alignment presented another critical aspect of our data preparation. We implemented proper
date handling and year extraction procedures, which proved essential for matching highway
infrastructure data with our crash records. This temporal standardization enables us to effectively
track changes in road infrastructure over time and establish meaningful correlations with crash
patterns. The alignment process ensures that our analysis captures the dynamic relationship between
infrastructure changes and safety outcomes.

Our quality validation process incorporated multiple layers of verification to ensure data integrity.
We conducted thorough examinations of data completeness across different years, verified the
consistency of measurements, analyzed the distribution of road types, and tracked year-over-year
changes in key metrics.

```{r, eval=FALSE, echo=TRUE}
# Function to standardize road classifications across different reporting formats
standardize_road_class <- function(road_class) {
  case_when(
    # Interstate highways (I-495, I-270, etc.)
    str_detect(road_class, "(?i)interstate|I-[0-9]") ~ "Interstate",
    
    # Other controlled-access highways (MD 200)
    str_detect(road_class, "(?i)freeway|expressway|controlled") ~ "Other Freeways",
    
    # Major arterial roads (Georgia Ave, Rockville Pike)
    str_detect(road_class, "(?i)principal.*arterial|major.*highway") ~ "Principal Arterial",
    
    # Secondary arterial roads
    str_detect(road_class, "(?i)minor.*arterial|secondary") ~ "Minor Arterial",
    
    # Major collector roads
    str_detect(road_class, "(?i)major.*collector|primary.*collector") ~ "Major Collector",
    
    # Minor collector roads
    str_detect(road_class, "(?i)minor.*collector|secondary.*collector") ~ "Minor Collector",
    
    # Local roads and streets
    str_detect(road_class, "(?i)local|residential|unclassified") ~ "Local Roads",
    
    # Default classification for undefined categories
    TRUE ~ "Other"
  )
}

# Process and clean highway mileage data
highway_data <- read_excel("Annual_Highway_Mileage.xlsx") %>%
  # Clean column names to consistent format
  janitor::clean_names() %>%
  
  # Filter specifically for Montgomery County data
  filter(county == "Montgomery") %>%
  
  # Handle missing values in critical columns
  mutate(
    aadt = replace_na(aadt, median(aadt, na.rm = TRUE)),
    lane_miles = replace_na(lane_miles, 0),
    
    # Create standardized classifications
    road_class = standardize_road_class(functional_class),
    urban_rural = if_else(str_detect(area_type, "(?i)urban"), "Urban", "Rural"),
    
    # Convert dates to proper format
    report_date = mdy(report_date),
    year = year(report_date)
  ) %>%
  
  # Calculate key metrics by road classification
  group_by(road_class, urban_rural, year) %>%
  summarize(
    # Total lane miles for each classification
    total_miles = sum(lane_miles, na.rm = TRUE),
    
    # Average daily traffic (weighted by segment length)
    avg_daily_traffic = weighted.mean(aadt, lane_miles, na.rm = TRUE),
    
    # Total vehicle miles traveled
    vehicle_miles_traveled = sum(vmt, na.rm = TRUE),
    
    # Number of segments in each classification
    segment_count = n(),
    
    # Additional metrics for analysis
    avg_segment_length = mean(lane_miles, na.rm = TRUE),
    total_intersections = sum(intersection_count, na.rm = TRUE)
  ) %>%
  ungroup()

# Data quality validation
quality_check <- highway_data %>%
  group_by(year) %>%
  summarize(
    # Basic completeness checks
    total_miles = sum(total_miles),
    missing_traffic = sum(is.na(avg_daily_traffic)),
    missing_vmt = sum(is.na(vehicle_miles_traveled)),
    
    # Consistency checks
    min_segment_length = min(avg_segment_length),
    max_segment_length = max(avg_segment_length),
    
    # Distribution checks
    urban_ratio = sum(urban_rural == "Urban") / n(),
    
    # Year-over-year change
    pct_change_miles = (total_miles - lag(total_miles)) / lag(total_miles) * 100
  )

# Save cleaned dataset with documentation
write_rds(highway_data, "cleaned_highway_data.rds")
write_csv(quality_check, "highway_data_quality_report.csv")
```